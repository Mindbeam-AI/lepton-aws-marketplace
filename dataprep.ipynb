{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3995494a-7d04-4486-b2a0-73f38bf7751e",
   "metadata": {},
   "source": [
    "# Dataset preparation for Lepton pre-training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "724e1357-a156-46a3-aba3-028f31412e3a",
   "metadata": {},
   "source": [
    "This notebook guides the subscriber how to download a dataset from Hugging Face, prepare the dataset with the Lepton tokenizer, and upload the prepared dataset to their S3 bucket for running a training job with the Mindbeam-Lepton pre-training algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db01f21-9bb5-47d5-880c-90f2f6c88190",
   "metadata": {},
   "source": [
    "## Install required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570dd075-4d0b-4f6e-bdd5-6bbe957325ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install jsonargparse pandas pyarrow gitpython boto3 tqdm numpy lightning tokenizers==0.19.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e41e5fa8-98f2-498f-a33a-de5927673d96",
   "metadata": {},
   "source": [
    "## Install git and git-lfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3032c3b-b445-4a41-9b8a-f50f09aa081d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !sudo apt-get install git git-lfs  # For Ubuntu/Debian\n",
    "# or\n",
    "# brew install git git-lfs     # For macOS\n",
    "# or\n",
    "# For Amazon Sagemaker  do below\n",
    "!curl -Lo /tmp/git-lfs.tar.gz https://github.com/git-lfs/git-lfs/releases/download/v3.4.1/git-lfs-linux-amd64-v3.4.1.tar.gz\n",
    "!tar -xf /tmp/git-lfs.tar.gz -C /tmp\n",
    "!sudo /tmp/git-lfs-3.4.1/install.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7495d52-46a8-473c-855a-d447a84c8439",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# check if git-lfs has been successfully installed\n",
    "!git lfs version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "737e1f72-b5dc-4b33-bc41-3fb0114b47ad",
   "metadata": {},
   "source": [
    "## Dataset preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f35506d0-8f50-4fe7-bd55-30ab2f6881de",
   "metadata": {},
   "source": [
    "First export your Hugging Face token for downloading the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07338410-af9f-4a49-8d35-8c0890ac6b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['HF_TOKEN'] = '<hf_your_token>'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b51281f-2cca-4586-854d-0dec848dcd07",
   "metadata": {},
   "source": [
    "Now run the dataset preparation script. Make sure that the `destination_path` is a local directory that has sufficient storage to store the prepared data. `s3_bucket` is the name of your S3 bucket, `s3_prefix` is the directory under the bucket where the prepared dataset is to be uploaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b551deb0-ea64-4b83-b3dc-0b7b3bf24cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m prepare_dataset.custom_preparer \\\n",
    "    \"<org_name>/<Dataset_name>\" \\\n",
    "    --destination_path /tmp/prepared_data \\\n",
    "    --tokenizer_path tokenizer/pints \\\n",
    "    --s3_bucket \"<my_s3_bucket_name>\" \\\n",
    "    --s3_prefix \"<dataset_directory_inside_s3_bucket>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84187c71-2b6c-43f6-baed-3c86a47ace2f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
